"""
Timeline Analyzer - Optimized for long audio
- Samples segments to a fixed upper bound
- Uses faster per-segment features (sentiment + toxicity + keywords)
- Dynamic bin sizing for long durations
"""

import math
from typing import List, Dict
from utils.nlp_analyzer import NLPAnalyzer
from utils.ensemble_scorer import EnsembleScorer
import os


class TimelineAnalyzer:
    """Creates timeline with real NLP-based segment scores (optimized)"""
    
    def __init__(self):
        self.nlp = NLPAnalyzer()
        self.scorer = EnsembleScorer()
        # allow override via env, default 240 sampled segments max
        self.MAX_SEGMENTS = int(os.getenv("MAX_TIMELINE_SEGMENTS", "240"))
    
    def _sample_segments(self, segments: List[Dict]) -> List[Dict]:
        """Downsample segments to at most MAX_SEGMENTS, evenly across time."""
        n = len(segments)
        if n <= self.MAX_SEGMENTS:
            return segments
        
        step = math.ceil(n / self.MAX_SEGMENTS)
        return segments[::step]  # take every k-th
    
    def analyze_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        Analyze each (sampled) segment with real NLP models.
        Heavy classifiers (zero-shot competency) are skipped per segment.
        """
        if not segments:
            return []
        
        # Load NLP models once
        self.nlp.load_models()
        
        # downsample aggressively on long calls
        segments = self._sample_segments(segments)
        
        scored_segments = []
        for i, segment in enumerate(segments, 1):
            txt = (segment.get('text') or '').strip()
            if len(txt) < 10:
                continue  # skip trivial fillers
            
            # Fast features only
            sentiment = self.nlp.analyze_sentiment(txt)
            toxicity = self.nlp.analyze_toxicity(txt)
            
            # very light keywords
            keywords = self.nlp.detect_keywords(
                txt,
                positive_keywords=["yes", "definitely", "experience", "achieved", "successfully"],
                negative_keywords=["um", "uh", "maybe", "I guess"]
            )
            
            # Compute a score using a neutral competency proxy (avoid zero-shot here)
            results = self.scorer.calculate_ensemble_score(
                sentiment_scores=sentiment,
                toxicity_score=toxicity["toxic"],
                competency_scores={"general": 50.0},  # neutral per-segment
                keyword_match=keywords
            )
            
            scored_segments.append({
                'start': segment.get('start', 0),
                'end': segment.get('end', 0),
                'text': txt,
                'score': results['score'],
                'sentiment': sentiment,
                'toxicity': toxicity,
                'prediction': results['prediction']
            })
        
        return scored_segments
    
    def create_timeline_data(self, scored_segments: List[Dict], duration: float) -> Dict:
        """
        Create timeline visualization data from scored segments.
        Bin size adapts to keep ~100 bins on very long calls.
        """
        if not scored_segments or duration <= 0:
            return {'bins': [], 'duration': 0, 'bin_size': 0}
        
        # target around ~100 bins; min 20s, max 120s
        target_bins = 100
        bin_size = max(20, min(120, int(max(1, duration // target_bins))))
        
        num_bins = int(duration / bin_size) + 1
        bins = []
        
        for i in range(num_bins):
            bin_start = i * bin_size
            bin_end = min((i + 1) * bin_size, duration)
            
            bin_segments = [
                seg for seg in scored_segments
                if seg['start'] >= bin_start and seg['start'] < bin_end
            ]
            
            if bin_segments:
                avg_score = sum(seg['score'] for seg in bin_segments) / len(bin_segments)
                if avg_score >= 70:
                    color, label = 'green', 'Strong'
                elif avg_score >= 40:
                    color, label = 'yellow', 'Okay'
                else:
                    color, label = 'red', 'Weak'
                
                bins.append({
                    'start': bin_start,
                    'end': bin_end,
                    'score': round(avg_score, 1),
                    'color': color,
                    'label': label,
                    'segment_count': len(bin_segments),
                    'segments': [
                        {
                            'text': seg['text'][:100] + '...' if len(seg['text']) > 100 else seg['text'],
                            'score': seg['score'],
                            'start': seg['start'],
                            'end': seg['end']
                        }
                        for seg in bin_segments
                    ]
                })
        
        return {
            'bins': bins,
            'duration': duration,
            'bin_size': bin_size
        }
